import json
from collections import Counter
from typing import Dict, List

def analyze_evaluation_results(file_path: str = 'evaluation_results.json'):
    """Ph√¢n t√≠ch chi ti·∫øt k·∫øt qu·∫£ evaluation"""
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    metrics = data['metrics']
    results = data['detailed_results']
    
    print("="*80)
    print("PH√ÇN T√çCH K·∫æT QU·∫¢ EVALUATION")
    print("="*80)
    
    # 1. T·ªïng quan metrics
    print("\nüìä T·ªîNG QUAN METRICS:")
    print("-" * 80)
    print(f"T·ªïng s·ªë c√¢u h·ªèi: {data['total_results']}")
    print(f"\nüìù Answer Task:")
    print(f"  ‚Ä¢ Accuracy:        {metrics['answer_task']['accuracy']:.1%} ({metrics['answer_task']['accuracy']*data['total_results']:.0f}/{data['total_results']})")
    print(f"  ‚Ä¢ Exact Match:     {metrics['answer_task']['exact_match']:.1%}")
    print(f"  ‚Ä¢ F1 Score:        {metrics['answer_task']['f1_score']:.3f}")
    print(f"  ‚Ä¢ Acc‚Ä† (Model):    {metrics['answer_task']['acc_dagger']:.1%}")
    print(f"\nüîç Retrieval Task:")
    print(f"  ‚Ä¢ Recall@3:        {metrics['retrieval_task']['recall_at_3']:.1%} ({metrics['retrieval_task']['recall_at_3']*data['total_results']:.0f}/{data['total_results']})")
    print(f"\n‚ö° Performance:")
    print(f"  ‚Ä¢ Avg Latency:     {metrics['performance']['avg_latency']:.2f}s")
    
    # 2. Ph√¢n t√≠ch c√°c lo·∫°i l·ªói
    print("\n" + "="*80)
    print("üîç PH√ÇN T√çCH C√ÅC LO·∫†I L·ªñI")
    print("="*80)
    
    correct_answers = []
    wrong_answers = []
    retrieval_failures = []
    answer_format_issues = []
    partial_correct = []
    
    for r in results:
        if r['answer_metrics']['accuracy'] == 1.0:
            correct_answers.append(r)
        else:
            wrong_answers.append(r)
            
            # Ph√¢n lo·∫°i l·ªói
            if r['retrieval_metrics']['recall_at_3'] == 0.0:
                retrieval_failures.append(r)
            
            if r['answer_metrics']['f1'] > 0.5 and r['answer_metrics']['f1'] < 1.0:
                partial_correct.append(r)
            
            # Ki·ªÉm tra format issues
            pred = r['prediction'].lower()
            if any(phrase in pred for phrase in ['kh√¥ng c√≥ th√¥ng tin', 'does not contain', 'the provided text']):
                answer_format_issues.append(r)
    
    print(f"\n‚úÖ C√¢u tr·∫£ l·ªùi ƒë√∫ng: {len(correct_answers)}/{data['total_results']} ({len(correct_answers)/data['total_results']:.1%})")
    print(f"‚ùå C√¢u tr·∫£ l·ªùi sai: {len(wrong_answers)}/{data['total_results']} ({len(wrong_answers)/data['total_results']:.1%})")
    print(f"\nüìä Ph√¢n lo·∫°i l·ªói:")
    print(f"  ‚Ä¢ Retrieval th·∫•t b·∫°i (Recall@3=0): {len(retrieval_failures)} ({len(retrieval_failures)/len(wrong_answers):.1%} c·ªßa c√°c l·ªói)")
    print(f"  ‚Ä¢ Partial correct (F1 > 0.5):      {len(partial_correct)} ({len(partial_correct)/len(wrong_answers):.1%} c·ªßa c√°c l·ªói)")
    print(f"  ‚Ä¢ Format issues (kh√¥ng c√≥ th√¥ng tin): {len(answer_format_issues)} ({len(answer_format_issues)/len(wrong_answers):.1%} c·ªßa c√°c l·ªói)")
    
    # 3. Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa Retrieval v√† Answer
    print("\n" + "="*80)
    print("üîó M·ªêI QUAN H·ªÜ GI·ªÆA RETRIEVAL V√Ä ANSWER")
    print("="*80)
    
    retrieval_success_answer_correct = 0
    retrieval_success_answer_wrong = 0
    retrieval_fail_answer_correct = 0
    retrieval_fail_answer_wrong = 0
    
    for r in results:
        recall = r['retrieval_metrics']['recall_at_3']
        accuracy = r['answer_metrics']['accuracy']
        
        if recall == 1.0 and accuracy == 1.0:
            retrieval_success_answer_correct += 1
        elif recall == 1.0 and accuracy == 0.0:
            retrieval_success_answer_wrong += 1
        elif recall == 0.0 and accuracy == 1.0:
            retrieval_fail_answer_correct += 1
        elif recall == 0.0 and accuracy == 0.0:
            retrieval_fail_answer_wrong += 1
    
    print(f"\nRetrieval th√†nh c√¥ng + Answer ƒë√∫ng:  {retrieval_success_answer_correct} ({retrieval_success_answer_correct/data['total_results']:.1%})")
    print(f"Retrieval th√†nh c√¥ng + Answer sai:   {retrieval_success_answer_wrong} ({retrieval_success_answer_wrong/data['total_results']:.1%})")
    print(f"Retrieval th·∫•t b·∫°i + Answer ƒë√∫ng:    {retrieval_fail_answer_correct} ({retrieval_fail_answer_correct/data['total_results']:.1%})")
    print(f"Retrieval th·∫•t b·∫°i + Answer sai:     {retrieval_fail_answer_wrong} ({retrieval_fail_answer_wrong/data['total_results']:.1%})")
    
    # 4. Ph√¢n t√≠ch c√°c v·∫•n ƒë·ªÅ c·ª• th·ªÉ
    print("\n" + "="*80)
    print("‚ö†Ô∏è  C√ÅC V·∫§N ƒê·ªÄ C·ª§ TH·ªÇ")
    print("="*80)
    
    # Format issues
    print(f"\n1. Format Issues ({len(answer_format_issues)} cases):")
    for i, r in enumerate(answer_format_issues[:5], 1):
        print(f"\n   Case {i}:")
        print(f"   Q: {r['question'][:100]}...")
        print(f"   GT: {r['ground_truth']}")
        print(f"   Pred: {r['prediction'][:100]}...")
        print(f"   Retrieval: Recall@3 = {r['retrieval_metrics']['recall_at_3']}")
    
    # Partial matches
    print(f"\n2. Partial Matches - G·∫ßn ƒë√∫ng nh∆∞ng format kh√°c ({len(partial_correct)} cases):")
    for i, r in enumerate(partial_correct[:5], 1):
        print(f"\n   Case {i}:")
        print(f"   Q: {r['question'][:80]}...")
        print(f"   GT: {r['ground_truth']}")
        print(f"   Pred: {r['prediction']}")
        print(f"   F1: {r['answer_metrics']['f1']:.3f}, Acc‚Ä†: {r['answer_metrics']['acc_dagger']}")
    
    # Retrieval failures
    print(f"\n3. Retrieval Failures ({len(retrieval_failures)} cases):")
    print(f"   - Khi retrieval th·∫•t b·∫°i, accuracy: {sum(1 for r in retrieval_failures if r['answer_metrics']['accuracy']==1.0)/len(retrieval_failures) if retrieval_failures else 0:.1%}")
    
    # 5. Ph√¢n t√≠ch theo F1 score ranges
    print("\n" + "="*80)
    print("üìà PH√ÇN T√çCH THEO F1 SCORE")
    print("="*80)
    
    f1_ranges = {
        "Perfect (F1=1.0)": [],
        "Good (0.7 <= F1 < 1.0)": [],
        "Fair (0.4 <= F1 < 0.7)": [],
        "Poor (0 < F1 < 0.4)": [],
        "Zero (F1=0)": []
    }
    
    for r in results:
        f1 = r['answer_metrics']['f1']
        if f1 == 1.0:
            f1_ranges["Perfect (F1=1.0)"].append(r)
        elif f1 >= 0.7:
            f1_ranges["Good (0.7 <= F1 < 1.0)"].append(r)
        elif f1 >= 0.4:
            f1_ranges["Fair (0.4 <= F1 < 0.7)"].append(r)
        elif f1 > 0:
            f1_ranges["Poor (0 < F1 < 0.4)"].append(r)
        else:
            f1_ranges["Zero (F1=0)"].append(r)
    
    for range_name, cases in f1_ranges.items():
        print(f"\n{range_name}: {len(cases)} cases ({len(cases)/data['total_results']:.1%})")
        if cases:
            avg_recall = sum(c['retrieval_metrics']['recall_at_3'] for c in cases) / len(cases)
            print(f"  ‚Ä¢ Avg Recall@3: {avg_recall:.1%}")
    
    # 6. Recommendations
    print("\n" + "="*80)
    print("üí° KHUY·∫æN NGH·ªä")
    print("="*80)
    
    print("\n1. Retrieval Task:")
    recall_rate = metrics['retrieval_task']['recall_at_3']
    if recall_rate < 0.5:
        print(f"   ‚ö†Ô∏è  Recall@3 th·∫•p ({recall_rate:.1%}) - C·∫ßn c·∫£i thi·ªán:")
        print(f"      ‚Ä¢ TƒÉng s·ªë l∆∞·ª£ng retrieved docs (k > 3)")
        print(f"      ‚Ä¢ C·∫£i thi·ªán embedding model ho·∫∑c fine-tuning")
        print(f"      ‚Ä¢ Th·ª≠ re-ranking")
    else:
        print(f"   ‚úÖ Recall@3 ·ªü m·ª©c ch·∫•p nh·∫≠n ƒë∆∞·ª£c ({recall_rate:.1%})")
    
    print("\n2. Answer Task:")
    accuracy = metrics['answer_task']['accuracy']
    if accuracy < 0.3:
        print(f"   ‚ö†Ô∏è  Accuracy r·∫•t th·∫•p ({accuracy:.1%}) - C√°c v·∫•n ƒë·ªÅ ch√≠nh:")
        print(f"      ‚Ä¢ {len(answer_format_issues)} cases: LLM tr·∫£ v·ªÅ 'kh√¥ng c√≥ th√¥ng tin'")
        print(f"      ‚Ä¢ {len(partial_correct)} cases: G·∫ßn ƒë√∫ng nh∆∞ng format kh√°c (F1 > 0.5)")
        print(f"      ‚Ä¢ C·∫ßn c·∫£i thi·ªán prompt ƒë·ªÉ LLM tr·∫£ v·ªÅ ƒë√∫ng format")
        print(f"      ‚Ä¢ C·∫ßn c·∫£i thi·ªán h√†m extract_final_answer ƒë·ªÉ x·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p edge case")
    
    print("\n3. C·∫£i thi·ªán c·ª• th·ªÉ:")
    if len(answer_format_issues) > 0:
        print(f"   ‚Ä¢ X·ª≠ l√Ω {len(answer_format_issues)} cases LLM n√≥i 'kh√¥ng c√≥ th√¥ng tin':")
        print(f"     - C·∫£i thi·ªán prompt ƒë·ªÉ bu·ªôc LLM tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi")
        print(f"     - TƒÉng ƒë·ªô d√†i intermediate_answer (hi·ªán t·∫°i 1000 chars)")
        print(f"     - Th·ª≠ retrieve nhi·ªÅu docs h∆°n v√† combine")
    
    if len(partial_correct) > 0:
        print(f"   ‚Ä¢ X·ª≠ l√Ω {len(partial_correct)} cases format kh√°c:")
        print(f"     - Normalize answer extraction (x·ª≠ l√Ω '05' vs '5', 'Ng√†y' vs 'ng√†y')")
        print(f"     - C·∫£i thi·ªán h√†m normalize_answer trong evaluator")
    
    print("\n" + "="*80)

if __name__ == "__main__":
    analyze_evaluation_results()
